{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57a9ec4-bb61-447e-a375-8f19970223c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "# Add the current directory to PYTHONPATH\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2eb0d2d-4204-4237-8af6-08558c730d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessorPipeline: \n",
    "    def __init__ (self):\n",
    "        # self.cat_encoder = None\n",
    "        self.scaler = None\n",
    "        self.imputers = {}\n",
    "        self.log = []\n",
    "\n",
    "    def drop_duplicates(self, df):\n",
    "        original_count = len(df)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        dropped_count = original_count - len(df)\n",
    "        self.log.append(f\"Dropped {dropped_count} duplicate rows.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def standardize_postcode(postcode):\n",
    "        # Standardizes a postcode to the format 'SW1A 1AA'\n",
    "        if isinstance(postcode, str):\n",
    "            postcode = postcode.strip().upper()\n",
    "            match = re.match(r'^([A-Z]{1,2}[0-9][A-Z0-9]?)(\\s*?)([0-9][A-Z]{2})$', postcode)\n",
    "            if match:\n",
    "                return f\"{match.group(1)} {match.group(3)}\"\n",
    "        return postcode\n",
    "\n",
    "    def merge_datasets(self, df, sector_data=None, station_data=None, district_data=None):\n",
    "        # Merge additional datasets into the main DataFrame\n",
    "        \n",
    "        if sector_data is not None and \"sector\" in df.columns:\n",
    "            df = df.merge(sector_data, on=\"sector\", how=\"left\")\n",
    "            self.log.append(\"Merged 'sector_data' into the main dataset.\")\n",
    "\n",
    "        if district_data is not None and \"postcodeDistrict\" in df.columns:\n",
    "            df = df.merge(district_data, on=\"postcodeDistrict\", how=\"left\")\n",
    "            self.log.append(\"Merged 'district_data' into the main dataset.\")\n",
    "\n",
    "        # Add distance to nearest station(Improves geospatial understanding of flood risks by correlating risk with monitored data.)\n",
    "        if station_data is not None:\n",
    "            station_coords = station_data[[\"latitude\", \"longitude\"]].to_numpy()\n",
    "            postcode_coords = df[[\"northing\", \"easting\"]].to_numpy()\n",
    "            distances = cdist(postcode_coords, station_coords, metric=\"euclidean\")\n",
    "            df[\"distance_to_station\"] = distances.min(axis=1)\n",
    "            self.log.append(\"Added 'distance_to_station' feature using station data.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def handle_missing_data(self, df, categorical_columns, numeric_columns, method=\"median\"):\n",
    "        # Handles missing data using the specified imputation strategy\n",
    "        \n",
    "        # Impute categorical columns with the mode\n",
    "        for col in categorical_columns:\n",
    "            if col in df.columns:\n",
    "                mode_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "                df[col] = mode_imputer.fit_transform(df[[col]])\n",
    "                self.imputers[col] = mode_imputer\n",
    "                self.log.append(f\"Imputed missing values in '{col}' using mode.\")\n",
    "\n",
    "        # Impute numeric columns\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                imputer = SimpleImputer(strategy=method)\n",
    "                df[col] = imputer.fit_transform(df[[col]])\n",
    "                self.imputers[col] = imputer\n",
    "                self.log.append(f\"Imputed missing values in '{col}' using {method} strategy.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "    def scale_numeric_features(self, df, numeric_columns, scaling_type=\"standard\"):\n",
    "        # Scales numeric features using the specified scaler\n",
    "        \n",
    "        if scaling_type == \"standard\":\n",
    "            self.scaler = StandardScaler()\n",
    "        elif scaling_type == \"minmax\":\n",
    "            self.scaler = MinMaxScaler()\n",
    "        elif scaling_type == 'robustscaler':\n",
    "            self.scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling_type. Use 'standard' or 'minmax'.\")\n",
    "\n",
    "        numeric_columns = [col for col in numeric_columns if col in df.columns]\n",
    "        if numeric_columns:\n",
    "            df[numeric_columns] = self.scaler.fit_transform(df[numeric_columns])\n",
    "            self.log.append(f\"Scaled numeric columns: {numeric_columns} using {scaling_type} scaling.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "    def feature_engineering(self, df, sector_data=None, station_data=None):\n",
    "        # Adds derived features like proximity risk and population density\n",
    "        \n",
    "        # Calculate proximity risk\n",
    "        if \"distanceToWatercourse\" in df.columns and \"elevation\" in df.columns:\n",
    "            df[\"proximity_risk\"] = df[\"distanceToWatercourse\"] / (df[\"elevation\"] + 1)\n",
    "            self.log.append(\"Added 'proximity_risk' feature.\")\n",
    "\n",
    "        # Add population density from sector data\n",
    "        if sector_data is not None and \"sector\" in df.columns:\n",
    "            df[\"population_density\"] = sector_data[\"population\"] / sector_data[\"households\"]\n",
    "            self.log.append(\"Added 'population_density' feature from sector data.\")\n",
    "        return df    \n",
    "\n",
    "    def interaction_general(self, df):\n",
    "        # Initialize label encoders\n",
    "        label_encoder_soil = LabelEncoder()\n",
    "        label_encoder_watercourse = LabelEncoder()\n",
    "\n",
    "        # Encode 'soilType' and 'nearestWatercourse'\n",
    "        df['soilType_encoded'] = label_encoder_soil.fit_transform(df['soilType'])\n",
    "        df['nearestWatercourse_encoded'] = label_encoder_watercourse.fit_transform(df['nearestWatercourse'])\n",
    "\n",
    "        # Calculate bins for elevation and distanceToWatercourse\n",
    "        percentiles = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "        bins_elevation = df['elevation'].quantile(percentiles).values\n",
    "        bins_distance = df['distanceToWatercourse'].quantile(percentiles).values\n",
    "\n",
    "        # Define bin labels\n",
    "        bin_labels = ['Low-Mid', 'Mid', 'Mid-High', 'High']\n",
    "\n",
    "         # Perform binning\n",
    "        df['elevation_category'] = pd.cut(df['elevation'], bins=bins_elevation, labels=bin_labels, include_lowest=True)\n",
    "        df['distanceToWatercourse_category'] = pd.cut(df['distanceToWatercourse'], bins=bins_distance, labels=bin_labels, include_lowest=True)\n",
    "    \n",
    "        # Combine categorical columns for interaction terms\n",
    "        df['soilType/Elevation'] = df['soilType_encoded'].astype(str) + '/' + df['elevation_category'].astype(str)\n",
    "        df['distanceToWatercourse/nearestWatercourse'] = df['distanceToWatercourse_category'].astype(str) + '/' + df['nearestWatercourse_encoded'].astype(str)\n",
    "  \n",
    "\n",
    "        # Apply label encoding to interaction columns\n",
    "        interaction_encoders = {col: LabelEncoder() for col in ['soilType/Elevation', 'distanceToWatercourse/nearestWatercourse' ]}\n",
    "        for col, encoder in interaction_encoders.items():\n",
    "            df[col] = encoder.fit_transform(df[col])\n",
    "\n",
    "        # Drop unnecessary intermediate encoded columns\n",
    "        df.drop(columns=['soilType_encoded', 'nearestWatercourse_encoded', 'elevation_category', 'distanceToWatercourse_category'], inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def preprocess(self, df, categorical_columns=None, numeric_columns=None, scaling_type=\"standard\",\n",
    "                   imputation_method=\"median\", sector_data=None, station_data=None, district_data=None):\n",
    "        # Executes the complete preprocessing pipeline.\n",
    "        \n",
    "        if categorical_columns is None:\n",
    "            categorical_columns = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        if numeric_columns is None:\n",
    "            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        # Standardize postcodes\n",
    "        if \"postcode\" in df.columns:\n",
    "            df[\"postcode\"] = df[\"postcode\"].apply(self.standardize_postcode)\n",
    "            self.log.append(\"Standardized 'postcode' column.\")\n",
    "\n",
    "        # Merge supporting datasets\n",
    "        df = self.merge_datasets(df, sector_data, station_data, district_data)\n",
    "\n",
    "        # Handle missing data\n",
    "        df = self.handle_missing_data(df, categorical_columns, numeric_columns, method=imputation_method)\n",
    "\n",
    "        # Encode categorical features\n",
    "        df = self.encode_categorical_features(df, categorical_columns)\n",
    "\n",
    "        # Feature engineering\n",
    "        df = self.feature_engineering(df, sector_data, station_data)\n",
    "\n",
    "        # Scale numeric features\n",
    "        df = self.scale_numeric_features(df, numeric_columns, scaling_type=scaling_type)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generates a summary of preprocessing actions.\"\"\"\n",
    "        return \"\\n\".join(self.log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
